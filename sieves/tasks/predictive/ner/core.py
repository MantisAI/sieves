from __future__ import annotations

from collections.abc import Iterable
from typing import Any, TypeAlias

import datasets
import pydantic

from sieves.data import Doc
from sieves.engines import Engine, EngineType, dspy_, glix_, huggingface_, instructor_, langchain_, ollama_, outlines_
from sieves.engines.core import EngineInferenceMode, EngineModel, EnginePromptSignature, EngineResult
from sieves.serialization import Config

from sieves.tasks.predictive.ner.bridges import (
    DSPyNER,
    InstructorNER,
    LangChainNER,
    OllamaNER,
    OutlinesNER,
    GliXNER,
)
from sieves.tasks.predictive.core import PredictiveTask

_TaskPromptSignature: TypeAlias = None
_TaskResult: TypeAlias = list[tuple[str, int, int]] | list[tuple[str, str, int, int]] | pydantic.BaseModel | dspy_.Result | glix_.Result | huggingface_.Result | instructor_.Result | langchain_.Result | ollama_.Result | outlines_.Result
_TaskBridge: TypeAlias = (
    None
)

class Entity(pydantic.BaseModel):
    text: str
    start: int
    end: int
    entity: str

class TaskFewshotExample(pydantic.BaseModel):
    text: str
    entities: list[Entity]


class NER(PredictiveTask[_TaskPromptSignature, _TaskResult, _TaskBridge]):
    def __init__(
            self,
            entities: list[str],
            engine: Engine[EnginePromptSignature, EngineResult, EngineModel, EngineInferenceMode],
            task_id: str | None = None,
            show_progress: bool = True,
            include_meta: bool = True,
            prompt_template: str | None = None,
            prompt_signature_desc: str | None = None,
            fewshot_examples: Iterable[TaskFewshotExample] = (),
    ) -> None:
        """"
        Initializes new PredictiveTask.
        :param task_id: Task ID.
        :param show_progress: Whether to show progress bar for processed documents.
        :param include_meta: Whether to include meta information generated by the task.
        :param prompt_template: Custom prompt template. If None, task's default template is being used.
        :param prompt_signature_desc: Custom prompt signature description. If None, default will be used.
        :param fewshot_examples: Few-shot examples.
        """
        self._entities = entities or ["PERSON", "LOCATION", "ORGANIZATION"]
        super().__init__(
            engine=engine,
            task_id=task_id,
            show_progress=show_progress,
            include_meta=include_meta,
            overwrite=False,
            prompt_template=prompt_template,
            prompt_signature_desc=prompt_signature_desc,
            fewshot_examples=fewshot_examples,
        ) 
        self._fewshot_examples: Iterable[TaskFewshotExample]    

    def _init_bridge(self, engine_type: EngineType) -> _TaskBridge:
        """Initialize bridge.
        :return: Engine task.
        :raises ValueError: If engine type is not supported.
        """
        bridge_types: dict[EngineType, type[_TaskBridge]] = {
            EngineType.langchain: LangChainNER,
            EngineType.ollama: OllamaNER,
            EngineType.outlines: OutlinesNER,
            EngineType.dspy: DSPyNER,
            EngineType.instructor: InstructorNER,
            EngineType.glix: GliXNER,
        }
        try:
            bridge_type = bridge_types[engine_type](
                task_id=self._task_id,
                prompt_template=self._custom_prompt_template,
                prompt_signature_desc=self._custom_prompt_signature_desc,
                entities=self._entities,
            )
        except KeyError as err:
            raise KeyError(f"Engine type {engine_type} is not supported by {self.__class__.__name__}.") from err

        return bridge_type

    @property
    def supports(self) -> set[EngineType]:
        return {
            EngineType.langchain,
            EngineType.ollama,
            EngineType.dspy,
            EngineType.outlines,
            EngineType.instructor,
            EngineType.glix,
        }
    
    def _validate_fewshot_examples(self) -> None:
        for fs_example in self._fewshot_examples or []:
            for entity in fs_example.entities:
                if entity.entity not in self._entities:
                    raise ValueError(f"Entity {entity.entity} not in {self._entities}.")
                if entity.start < 0 or entity.end < 0:
                    raise ValueError(f"Entity {entity.entity} has start or end less than 0.")
                if entity.start > entity.end:
                    raise ValueError(f"Entity {entity.entity} has start greater than end.")
                if entity.start >= len(fs_example.text) or entity.end >= len(fs_example.text):
                    raise ValueError(f"Entity {entity.entity} has start or end greater than the text length.")
        
    @property
    def _state(self) -> dict[str, Any]:
        return {
            **super()._state,
            "entities": self._entities,
        }

    def to_dataset(self, docs: Iterable[Doc]) -> datasets.Dataset:
        """Convert NER results to a dataset.
        
        :param docs: Documents with NER results.
        :return: Dataset with NER results.
        """
        # Define metadata and features for the dataset
        features = datasets.Features({
            "text": datasets.Value("string"),
            "entities": datasets.Sequence({
                "text": datasets.Value("string"),
                "start": datasets.Value("int32"),
                "end": datasets.Value("int32"),
                "entity": datasets.Value("string")
            })
        })
        
        info = datasets.DatasetInfo(
            description=f"Named Entity Recognition dataset with entity types {self._entities}. Generated with sieves "
            f"v{Config.get_version()}.",
            features=features,
        )

        # Fetch data used for generating dataset
        try:
            data = []
            for doc in docs:
                if self._task_id not in doc.results:
                    raise KeyError(f"Document does not have results for task ID {self._task_id}")
                
                # Get the entities from the document results
                result = doc.results[self._task_id]
                entities = []
                
                # Handle different result formats
                if hasattr(result, 'entities'):
                    # Pydantic model format
                    for entity in result.entities:
                        entities.append({
                            "text": entity.text,
                            "start": entity.start,
                            "end": entity.end,
                            "entity": entity.entity
                        })
                elif isinstance(result, list):
                    # List format (could be list of dictionaries or other entities)
                    for entity in result:
                        if isinstance(entity, dict) and "text" in entity:
                            # Dictionary format with text key
                            entity_dict = {
                                "text": entity["text"],
                                "start": entity.get("start", 0),
                                "end": entity.get("end", len(entity["text"])),
                                "entity": entity.get("label", entity.get("entity", "UNKNOWN"))
                            }
                            entities.append(entity_dict)
                
                data.append((doc.text, entities))
        except KeyError as err:
            raise KeyError(f"Not all documents have results for this task with ID {self._task_id}") from err

        def generate_data() -> Iterable[dict[str, Any]]:
            """Yields results as dicts.
            :return: Results as dicts.
            """
            for text, entities in data:
                yield {"text": text, "entities": entities}

        # Create dataset
        return datasets.Dataset.from_generator(generate_data, features=features, info=info)