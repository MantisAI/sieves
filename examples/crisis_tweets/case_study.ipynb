{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Crisis NLP Analysis with Sieves**\n",
        "\n",
        "<img src=\"assets/sieves_logo.png\" width=200px height=200px />\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "1. [Imports](#imports)\n",
        "2. [Dataset Loading](#dataset-loading)\n",
        "3. [Data Exploration](#data-exploration)  \n",
        "4. [Creating Sieves Doc Objects](#creating-sieves-doc-objects)\n",
        "5. [Defining Models](#defining-models)\n",
        "6. [Language Detection](#language-detection)\n",
        "7. [Main Pipeline](#main-pipeline)\n",
        "8. [Evaluating Results](#evaluating-results)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Imports"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dependencies:\n",
        "ipykernel\n",
        "pandas\n",
        "openai\n",
        "outlines\n",
        "sieves\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.519421Z",
          "start_time": "2025-11-05T17:18:48.280919Z"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import openai\n",
        "import outlines\n",
        "from sieves import Pipeline, tasks, Doc, GenerationSettings\n",
        "from sieves.tasks.predictive.classification import FewshotExampleMultiLabel\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "import random\n",
        "\n",
        "# Number of rows per crisis type to subset to\n",
        "n_rows = 100\n",
        "\n",
        "# Subset of tweets\n",
        "n_tweets = 100\n"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/climatiq/dev/sieves/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
            "/Users/climatiq/dev/sieves/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the Dataset. If you are unable to run the following code block you can download manually the dataset with this url:\n",
        "\n",
        "https://crisisnlp.qcri.org/data/lrec2016/labeled_cf/CrisisNLP_labeled_data_crowdflower_v2.zip\n",
        "\n",
        "Then you can place is in a directory with path /examples/use-case/CrisisNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.636153Z",
          "start_time": "2025-11-05T17:19:04.609268Z"
        }
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "url = \"https://crisisnlp.qcri.org/data/lrec2016/labeled_cf/CrisisNLP_labeled_data_crowdflower_v2.zip\"\n",
        "zip_path = Path(\"CrisisNLP_dataset.zip\")\n",
        "data_dir = Path(\"CrisisNLP\")\n",
        "\n",
        "if not zip_path.exists():\n",
        "    urllib.request.urlretrieve(url, str(zip_path))\n",
        "\n",
        "with zipfile.ZipFile(str(zip_path), 'r') as z:\n",
        "    z.extractall(\".\")\n",
        "\n",
        "extracted = Path(\"CrisisNLP_labeled_data_crowdflower\")\n",
        "if extracted.exists():\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    for item in extracted.iterdir():\n",
        "        target = data_dir / item.name\n",
        "        if not target.exists():\n",
        "            item.rename(target)\n",
        "    shutil.rmtree(extracted)\n"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.698633Z",
          "start_time": "2025-11-05T17:19:04.642479Z"
        }
      },
      "source": [
        "data: pd.DataFrame = pd.DataFrame()\n",
        "\n",
        "print(\"Available Crisis Types:\\n\")\n",
        "for file in os.listdir(\"CrisisNLP\"):\n",
        "    if os.path.isdir(os.path.join(\"CrisisNLP\", file)):\n",
        "        print(f\"- {file}\")\n",
        "        for file_ in os.listdir(os.path.join(\"CrisisNLP\", file)):\n",
        "            if file_.endswith(\".tsv\"):\n",
        "                data_ = pd.read_csv(os.path.join(\"CrisisNLP\", file, file_), sep=\"\\t\")\n",
        "                \n",
        "                if n_rows is not None:\n",
        "                    # Subset to n_rows rows for testing\n",
        "                    data_ = data_[0:n_rows]\n",
        "                \n",
        "                data_[\"location\"] = file.split(\"_\")[0]\n",
        "                data_[\"crisis_type\"] = file.split(\"_\")[1]\n",
        "                data = pd.concat([data, data_])\n",
        "\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Crisis Types:\n",
            "\n",
            "- 2015_Nepal_Earthquake_en\n",
            "- 2015_Cyclone_Pam_en\n",
            "- 2014_Middle_East_Respiratory_Syndrome_en\n",
            "- 2014_India_floods\n",
            "- 2014_Pakistan_floods\n",
            "- 2014_ebola_cf\n",
            "- 2014_California_Earthquake\n",
            "- 2013_Pakistan_eq\n",
            "- 2014_Hurricane_Odile_Mexico_en\n",
            "- 2014_Chile_Earthquake_cl\n",
            "- 2014_Chile_Earthquake_en\n",
            "- 2014_Philippines_Typhoon_Hagupit_en\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.715751Z",
          "start_time": "2025-11-05T17:19:04.707948Z"
        }
      },
      "source": [
        "data.tail()"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "               tweet_id                                         tweet_text  \\\n",
              "5  '541562943847272449'  RT @rapplerdotcom: #RubyPH cuts power lines in...   \n",
              "6  '541246399426625536'  3Novices:Super Typhoon Lashes Philippines as 6...   \n",
              "7  '541210384800030720'  #TeamYamita Hundreds of Thousands Evacuated in...   \n",
              "8  '541610392825634816'  CALL FOR VOLUNTEERS: Repacking of #RubyPH reli...   \n",
              "9  '540796767663439872'  RT @ChannelNewsAsia: Filipinos seek shelter in...   \n",
              "\n",
              "                                               label location  crisis_type  \n",
              "5                infrastructure_and_utilities_damage     2014  Philippines  \n",
              "6                   displaced_people_and_evacuations     2014  Philippines  \n",
              "7                   displaced_people_and_evacuations     2014  Philippines  \n",
              "8  donation_needs_or_offers_or_volunteering_services     2014  Philippines  \n",
              "9                   displaced_people_and_evacuations     2014  Philippines  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>label</th>\n",
              "      <th>location</th>\n",
              "      <th>crisis_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>'541562943847272449'</td>\n",
              "      <td>RT @rapplerdotcom: #RubyPH cuts power lines in...</td>\n",
              "      <td>infrastructure_and_utilities_damage</td>\n",
              "      <td>2014</td>\n",
              "      <td>Philippines</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>'541246399426625536'</td>\n",
              "      <td>3Novices:Super Typhoon Lashes Philippines as 6...</td>\n",
              "      <td>displaced_people_and_evacuations</td>\n",
              "      <td>2014</td>\n",
              "      <td>Philippines</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>'541210384800030720'</td>\n",
              "      <td>#TeamYamita Hundreds of Thousands Evacuated in...</td>\n",
              "      <td>displaced_people_and_evacuations</td>\n",
              "      <td>2014</td>\n",
              "      <td>Philippines</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'541610392825634816'</td>\n",
              "      <td>CALL FOR VOLUNTEERS: Repacking of #RubyPH reli...</td>\n",
              "      <td>donation_needs_or_offers_or_volunteering_services</td>\n",
              "      <td>2014</td>\n",
              "      <td>Philippines</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>'540796767663439872'</td>\n",
              "      <td>RT @ChannelNewsAsia: Filipinos seek shelter in...</td>\n",
              "      <td>displaced_people_and_evacuations</td>\n",
              "      <td>2014</td>\n",
              "      <td>Philippines</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.745045Z",
          "start_time": "2025-11-05T17:19:04.742338Z"
        }
      },
      "source": "data[\"location\"].value_counts()",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "location\n",
              "2014    90\n",
              "2015    20\n",
              "2013    10\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.787590Z",
          "start_time": "2025-11-05T17:19:04.781911Z"
        }
      },
      "source": [
        "print(\"Number of rows:\", len(data))\n",
        "print(\"Number of columns:\", len(data.columns))\n",
        "print(\"Column names:\", data.columns.tolist())\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "display(data.describe(include='all'))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows: 120\n",
            "Number of columns: 5\n",
            "Column names: ['tweet_id', 'tweet_text', 'label', 'location', 'crisis_type']\n",
            "\n",
            "Summary statistics:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "                    tweet_id  \\\n",
              "count                    120   \n",
              "unique                   120   \n",
              "top     '592326564110585856'   \n",
              "freq                       1   \n",
              "\n",
              "                                               tweet_text  \\\n",
              "count                                                 120   \n",
              "unique                                                120   \n",
              "top     RT @divyaconnects: Reached #Kathmandu finally!...   \n",
              "freq                                                    1   \n",
              "\n",
              "                           label location crisis_type  \n",
              "count                        120      120         120  \n",
              "unique                        14        3          10  \n",
              "top     other_useful_information     2014    Pakistan  \n",
              "freq                          33       90          20  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>label</th>\n",
              "      <th>location</th>\n",
              "      <th>crisis_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>'592326564110585856'</td>\n",
              "      <td>RT @divyaconnects: Reached #Kathmandu finally!...</td>\n",
              "      <td>other_useful_information</td>\n",
              "      <td>2014</td>\n",
              "      <td>Pakistan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>90</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Sieves Doc Objects\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.860068Z",
          "start_time": "2025-11-05T17:19:04.855109Z"
        }
      },
      "source": [
        "# Convert tweets to Sieves Doc objects\n",
        "def create_sieves_docs(df):\n",
        "    \"\"\"Create Sieves Doc objects from DataFrame\"\"\"\n",
        "    docs: Doc = []\n",
        "    for idx, row in df.iterrows():\n",
        "        doc = Doc(\n",
        "            uri=f\"tweet_{row['tweet_id']}\",\n",
        "            text=row['tweet_text']\n",
        "        )\n",
        "        # Add metadata\n",
        "        doc.metadata = {\n",
        "            'tweet_id': row['tweet_id'],\n",
        "            'gold_label': row['label'],\n",
        "            'location': row['location'],\n",
        "            'crisis_type': row['crisis_type']\n",
        "        }\n",
        "        docs.append(doc)\n",
        "    return docs\n",
        "\n",
        "# Create docs from English tweets\n",
        "docs = create_sieves_docs(data)\n",
        "print(f\"Created {len(docs)} Sieves Doc objects\")\n",
        "print(f\"\\nSample doc:\")\n",
        "print(f\"Tweet: {docs[0].text[:100]}...\")\n",
        "print(f\"Metadata: {docs[0].metadata}\")\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 120 Sieves Doc objects\n",
            "\n",
            "Sample doc:\n",
            "Tweet: RT @divyaconnects: Reached #Kathmandu finally! Lots of Indians stranded at the airport #NepalQuake @...\n",
            "Metadata: {'tweet_id': \"'592326564110585856'\", 'gold_label': 'other_useful_information', 'location': '2015', 'crisis_type': 'Nepal'}\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.920022Z",
          "start_time": "2025-11-05T17:19:04.902415Z"
        }
      },
      "source": [
        "# Using OpenAI models via outlines\n",
        "\n",
        "openrouter_client = openai.OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        ")\n",
        "# model = outlines.from_openai(openrouter_client, model_name=\"openai/gpt-5-nano\")\n",
        "model = outlines.from_openai(openrouter_client, model_name=\"google/gemini-2.5-flash-lite-preview-09-2025\")\n"
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Subset of docs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.930259Z",
          "start_time": "2025-11-05T17:19:04.928653Z"
        }
      },
      "source": [
        "if n_tweets is not None:\n",
        "    test_docs = random.sample(docs,n_tweets)\n",
        "else:\n",
        "    test_docs = docs"
      ],
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Language Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the language detection few shot examples, task and pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.958103Z",
          "start_time": "2025-11-05T17:19:04.955496Z"
        }
      },
      "source": [
        "from sieves.tasks.predictive.classification import FewshotExampleMultiLabel\n",
        "\n",
        "fewshot_examples = [\n",
        "    FewshotExampleMultiLabel(\n",
        "        text=\"Help! There's a major earthquake in California. Everyone stay safe!\",\n",
        "        reasoning=\"This text is clearly written in English with standard English words, grammar, and sentence structure.\",\n",
        "        confidence_per_label={\"english\": 1.0, \"other\": 0.0}\n",
        "    ),\n",
        "    FewshotExampleMultiLabel(\n",
        "        text=\"¡Ayuda! Hay un terremoto en México. ¡Manténganse seguros!\",\n",
        "        reasoning=\"This text is written in Spanish, using Spanish words like 'Ayuda' (help) and 'terremoto' (earthquake), and Spanish grammar structures.\",\n",
        "        confidence_per_label={\"english\": 0.0, \"other\": 1.0}\n",
        "    ),\n",
        "    FewshotExampleMultiLabel(\n",
        "        text=\"Praying for the victims of the flood in Nepal. #PrayForNepal\",\n",
        "        reasoning=\"This is an English tweet with standard English vocabulary, grammar, and hashtag format commonly used on Twitter.\",\n",
        "        confidence_per_label={\"english\": 0.95, \"other\": 0.05}\n",
        "    ),\n",
        "    FewshotExampleMultiLabel(\n",
        "        text=\"Bonjour! Comment allez-vous? Je parle français.\",\n",
        "        reasoning=\"This text contains French words and phrases. While it has some basic structure, it's clearly not English.\",\n",
        "        confidence_per_label={\"english\": 0.1, \"other\": 0.9}\n",
        "    ),\n",
        "    FewshotExampleMultiLabel(\n",
        "        text=\"RT @user: Just saw the news about the hurricane. Hope everyone is okay!\",\n",
        "        reasoning=\"This is an English tweet with typical Twitter retweet format (RT), English words, and standard English sentence structure.\",\n",
        "        confidence_per_label={\"english\": 0.98, \"other\": 0.02}\n",
        "    ),\n",
        "    FewshotExampleMultiLabel(\n",
        "        text=\"Hello world! This is a test message with some English words but also 你好世界\",\n",
        "        reasoning=\"This text contains both English and Chinese characters. It's mixed language, so while English is present, it's not entirely English.\",\n",
        "        confidence_per_label={\"english\": 0.6, \"other\": 0.7}\n",
        "    )\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:04.976066Z",
          "start_time": "2025-11-05T17:19:04.973062Z"
        }
      },
      "source": [
        "from sieves import GenerationSettings\n",
        "\n",
        "language_detector = tasks.Classification(\n",
        "        task_id=\"language_detector\",\n",
        "        labels=[\"english\", \"other\"],\n",
        "        model=model,\n",
        "        label_descriptions={\"english\": \"The tweet is in English.\", \"other\": \"The tweet is in a language other than English.\"},\n",
        "        prompt_instructions=\"\"\"\n",
        "        You are a helpful assistant that determines the language of a given text. If the text is in English, return 'english'. Otherwise, return 'other'.\n",
        "        \"\"\",\n",
        "        fewshot_examples=fewshot_examples,\n",
        "        batch_size=10,\n",
        "        generation_settings=GenerationSettings(strict_mode=True)\n",
        "    )\n",
        "\n",
        "language_detection = Pipeline([language_detector])\n",
        "print(f\"Number of tasks: {len(language_detection.tasks)}\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tasks: 1\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the language detection pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:19.835368Z",
          "start_time": "2025-11-05T17:19:04.991799Z"
        }
      },
      "source": [
        "try:\n",
        "    language_detection_results = list(language_detection(test_docs))\n",
        "    print(\"Language detection pipeline run successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error running language detection: {e}\")\n",
        "    raise e"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running pipeline: 100%|██████████| 10/10 [00:14<00:00,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language detection pipeline run successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:19.862919Z",
          "start_time": "2025-11-05T17:19:19.859967Z"
        }
      },
      "source": "language_detection_results[:10]",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Doc(meta={}, results={'language_detector': [('english', 0.95), ('other', 0.05)]}, uri=\"tweet_'468077392955990016'\", text='(#Luke 21:11) #MERS: Saudis + 3 more deaths, WHO says increase prevention http://t.co/1vTnlGeO4k #EndTimes #WorldNews http://t.co/cwBbDeMPAz', chunks=['(#Luke 21:11) #MERS: Saudis + 3 more deaths, WHO says increase prevention http://t.co/1vTnlGeO4k #EndTimes #WorldNews http://t.co/cwBbDeMPAz'], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('english', 1.0), ('other', 0.0)]}, uri=\"tweet_'592955183753203715'\", text='VIDEO REPORT Kathmandu overwhelmed by rubble after earthquake http://t.co/cYrAbG1OxZ Over 3000 dead, 6,500 plus injured #NepalEarthquake', chunks=['VIDEO REPORT Kathmandu overwhelmed by rubble after earthquake http://t.co/cYrAbG1OxZ Over 3000 dead, 6,500 plus injured #NepalEarthquake'], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('english', 0.9), ('other', 0.1)]}, uri=\"tweet_'383790723222364161'\", text='#Earthquake 2013-09-28 02:39:43 (M5.0) EAST OF THE SOUTH SANDWICH ISLANDS -59.5 -19.1 (70fa9) http://t.co/uBN98fFmNj notice', chunks=['#Earthquake 2013-09-28 02:39:43 (M5.0) EAST OF THE SOUTH SANDWICH ISLANDS -59.5 -19.1 (70fa9) http://t.co/uBN98fFmNj notice'], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('other', 0.95), ('english', 0.05)]}, uri=\"tweet_'451538561956065280'\", text='RT @ojsar: Arriba #Chile Dios con ustedes @24HorasTVN @TVChileTVN abrazos y oraciones a los afectados. Amo Chile', chunks=['RT @ojsar: Arriba #Chile Dios con ustedes @24HorasTVN @TVChileTVN abrazos y oraciones a los afectados. Amo Chile'], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('english', 0.98), ('other', 0.02)]}, uri=\"tweet_'576754146944163840'\", text='RT @WFP_Asia: In contact with #vanuatu to understand impact of #CyclonePam, @WFP staff on way for assessment of food, logistics needs.', chunks=['RT @WFP_Asia: In contact with #vanuatu to understand impact of #CyclonePam, @WFP staff on way for assessment of food, logistics needs.'], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('other', 0.85), ('english', 0.55)]}, uri=\"tweet_'451253361501675520'\", text='RT @jarekkuzniar: Nowe ZDJĘCIA z #Chile “@RT_com: Damage &amp; fires in northern #Chile following 8.2 quake http://t.co/6pXO7G4VqV http://t.co/…', chunks=['RT @jarekkuzniar: Nowe ZDJĘCIA z #Chile “@RT_com: Damage &amp; fires in northern #Chile following 8.2 quake http://t.co/6pXO7G4VqV http://t.co/…'], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('english', 0.97), ('other', 0.03)]}, uri=\"tweet_'514627577831768064'\", text='(09/22) ONE WAY TO HELP ODILE VICTIMS  #surfing http://t.co/mUjnzMgQqw', chunks=['(09/22) ONE WAY TO HELP ODILE VICTIMS  #surfing http://t.co/mUjnzMgQqw'], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('english', 1.0), ('other', 0.0)]}, uri=\"tweet_'503765744333901824'\", text=\"Say what you want, but the earthquake that hit California was all San Andrea's fault.\", chunks=[\"Say what you want, but the earthquake that hit California was all San Andrea's fault.\"], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('english', 0.99), ('other', 0.01)]}, uri=\"tweet_'523225601910775808'\", text='RT @itsmenanice: Read this. From Miasma to #Ebola: The History of Racist Moral Panic Over Disease http://t.co/H4ZCfCLYVx http://t.co/u5P3hU…', chunks=['RT @itsmenanice: Read this. From Miasma to #Ebola: The History of Racist Moral Panic Over Disease http://t.co/H4ZCfCLYVx http://t.co/u5P3hU…'], id=None, images=None),\n",
              " Doc(meta={}, results={'language_detector': [('english', 0.99), ('other', 0.01)]}, uri=\"tweet_'451585608612208640'\", text='Guys northem chile really needs a support message #PrayForChile', chunks=['Guys northem chile really needs a support message #PrayForChile'], id=None, images=None)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filter for english tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:19.886231Z",
          "start_time": "2025-11-05T17:19:19.883587Z"
        }
      },
      "source": [
        "# Filter for English tweets with confidence > 0.6\n",
        "english_docs = [\n",
        "    doc for doc in language_detection_results \n",
        "    if doc.results[\"language_detector\"][0][0] == \"english\" and doc.results[\"language_detector\"][0][1] > 0.6\n",
        "    or doc.results[\"language_detector\"][1][0] == \"english\" and doc.results[\"language_detector\"][1][1] > 0.6\n",
        "]\n",
        "\n",
        "print(f\"Number of English tweets: {len(english_docs)}\")\n",
        "print(f\"Number of non-English tweets: {len(language_detection_results) - len(english_docs)}\")\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of English tweets: 8\n",
            "Number of non-English tweets: 2\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:19.914805Z",
          "start_time": "2025-11-05T17:19:19.911377Z"
        }
      },
      "source": [
        "gen_settings = GenerationSettings(strict_mode=True)\n",
        "\n",
        "# Zero-shot taskswith default configuration\n",
        "class Country(BaseModel, frozen=True):\n",
        "    name: Literal[\"India\", \"California\", \"Pakistan\", \"Mexico\", \"Senegal\", \"Vanuatu\", \"Nepal\", \"Philippines\", \"Unknown\"] = Field(description=\"The name of the country mentioned in the tweet if any. If not, return 'unknown'\")\n",
        "\n",
        "class CrisisType(BaseModel, frozen=True):\n",
        "    crisis_type: Literal[\"Earthquake\", \"Flood\", \"Hurricane\", \"Typhoon\", \"Cyclone\", \"Ebola\", \"Unknown\"] = Field(description=\"The type of crisis mentioned in the tweet\")\n",
        "\n",
        "crisis_classifier = tasks.Classification(\n",
        "        task_id=\"crisis_classifier\",\n",
        "        labels=[\"related_to_crisis\", \"not_related_to_crisis\"],\n",
        "        model=model,\n",
        "        batch_size=10,\n",
        "        generation_settings=gen_settings\n",
        "    )\n",
        "\n",
        "crisis_type_extractor = tasks.InformationExtraction(\n",
        "        task_id=\"crisis_type_extractor\",\n",
        "        entity_type=CrisisType,\n",
        "        model=model,\n",
        "        batch_size=10,\n",
        "        generation_settings=gen_settings\n",
        "    )\n",
        "\n",
        "location_extractor = tasks.InformationExtraction(\n",
        "        task_id=\"location_extractor\",\n",
        "        entity_type=Country,\n",
        "        model=model,\n",
        "        batch_size=10,\n",
        "        generation_settings=gen_settings\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Main Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:19.930747Z",
          "start_time": "2025-11-05T17:19:19.928797Z"
        }
      },
      "source": [
        "pipe = Pipeline([\n",
        "    crisis_classifier,\n",
        "    crisis_type_extractor,\n",
        "    location_extractor,\n",
        "])\n",
        "\n",
        "print(\"Pipeline created successfully!\")\n",
        "print(f\"Number of tasks: {len(pipe.tasks)}\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline created successfully!\n",
            "Number of tasks: 3\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:42.781452Z",
          "start_time": "2025-11-05T17:19:19.940836Z"
        }
      },
      "source": [
        "try:\n",
        "    results = list(pipe(english_docs))\n",
        "    \n",
        "    # Display sample results\n",
        "    for i, doc in enumerate(results):\n",
        "        print(f\"\\\\n--- Tweet {i+1} ---\")\n",
        "        print(f\"Text: {doc.text}...\")\n",
        "        print(f\"Crisis Classifier: {doc.results.get('crisis_classifier', 'N/A')}\")\n",
        "        print(f\"Crisis Type Extraction: {doc.results.get('crisis_type_extractor')}\")\n",
        "        print(f\"Locations Found: {doc.results.get('location_extractor', 'N/A')}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error running pipeline: {e}\")\n"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running pipeline: 100%|██████████| 8/8 [00:22<00:00,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Tweet 1 ---\n",
            "Text: (#Luke 21:11) #MERS: Saudis + 3 more deaths, WHO says increase prevention http://t.co/1vTnlGeO4k #EndTimes #WorldNews http://t.co/cwBbDeMPAz...\n",
            "Crisis Classifier: [('related_to_crisis', 1.0), ('not_related_to_crisis', 0.0)]\n",
            "Crisis Type Extraction: [CrisisType(crisis_type='Unknown')]\n",
            "Locations Found: [Country(name='Unknown')]\n",
            "\\n--- Tweet 2 ---\n",
            "Text: VIDEO REPORT Kathmandu overwhelmed by rubble after earthquake http://t.co/cYrAbG1OxZ Over 3000 dead, 6,500 plus injured #NepalEarthquake...\n",
            "Crisis Classifier: [('related_to_crisis', 1.0), ('not_related_to_crisis', 0.0)]\n",
            "Crisis Type Extraction: [CrisisType(crisis_type='Earthquake')]\n",
            "Locations Found: [Country(name='Nepal')]\n",
            "\\n--- Tweet 3 ---\n",
            "Text: #Earthquake 2013-09-28 02:39:43 (M5.0) EAST OF THE SOUTH SANDWICH ISLANDS -59.5 -19.1 (70fa9) http://t.co/uBN98fFmNj notice...\n",
            "Crisis Classifier: [('related_to_crisis', 1.0), ('not_related_to_crisis', 0.0)]\n",
            "Crisis Type Extraction: [CrisisType(crisis_type='Earthquake')]\n",
            "Locations Found: [Country(name='Unknown')]\n",
            "\\n--- Tweet 4 ---\n",
            "Text: RT @WFP_Asia: In contact with #vanuatu to understand impact of #CyclonePam, @WFP staff on way for assessment of food, logistics needs....\n",
            "Crisis Classifier: [('related_to_crisis', 1.0), ('not_related_to_crisis', 0.0)]\n",
            "Crisis Type Extraction: [CrisisType(crisis_type='Cyclone')]\n",
            "Locations Found: [Country(name='Vanuatu')]\n",
            "\\n--- Tweet 5 ---\n",
            "Text: (09/22) ONE WAY TO HELP ODILE VICTIMS  #surfing http://t.co/mUjnzMgQqw...\n",
            "Crisis Classifier: [('related_to_crisis', 0.98), ('not_related_to_crisis', 0.02)]\n",
            "Crisis Type Extraction: [CrisisType(crisis_type='Hurricane')]\n",
            "Locations Found: [Country(name='Unknown')]\n",
            "\\n--- Tweet 6 ---\n",
            "Text: Say what you want, but the earthquake that hit California was all San Andrea's fault....\n",
            "Crisis Classifier: [('related_to_crisis', 1.0), ('not_related_to_crisis', 0.0)]\n",
            "Crisis Type Extraction: [CrisisType(crisis_type='Earthquake')]\n",
            "Locations Found: [Country(name='California')]\n",
            "\\n--- Tweet 7 ---\n",
            "Text: RT @itsmenanice: Read this. From Miasma to #Ebola: The History of Racist Moral Panic Over Disease http://t.co/H4ZCfCLYVx http://t.co/u5P3hU…...\n",
            "Crisis Classifier: [('related_to_crisis', 1.0), ('not_related_to_crisis', 0.0)]\n",
            "Crisis Type Extraction: [CrisisType(crisis_type='Ebola')]\n",
            "Locations Found: [Country(name='Unknown')]\n",
            "\\n--- Tweet 8 ---\n",
            "Text: Guys northem chile really needs a support message #PrayForChile...\n",
            "Crisis Classifier: [('related_to_crisis', 1.0), ('not_related_to_crisis', 0.0)]\n",
            "Crisis Type Extraction: [CrisisType(crisis_type='Earthquake')]\n",
            "Locations Found: [Country(name='Unknown')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a structured DataFrame with the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:42.802528Z",
          "start_time": "2025-11-05T17:19:42.797423Z"
        }
      },
      "source": [
        "evaluation_data = []\n",
        "for i, doc in enumerate(results):\n",
        "    # Get crisis classification\n",
        "    crisis_result = doc.results.get('crisis_classifier', [])\n",
        "    predicted_label = crisis_result[0][0] if crisis_result else 'unknown'\n",
        "    predicted_conf = crisis_result[0][1] if crisis_result else 0.0\n",
        "    \n",
        "    # Get crisis type extraction\n",
        "    crisis_type_result = doc.results.get('crisis_type_extractor', [])\n",
        "    crisis_type = crisis_type_result[0].crisis_type if crisis_type_result else 'none_extracted'\n",
        "    \n",
        "    # Get location extraction\n",
        "    location_result = doc.results.get('location_extractor', [])\n",
        "    locations = [loc.name for loc in location_result] if location_result else []\n",
        "    \n",
        "    # Get gold labels from metadata\n",
        "    gold_label = doc.metadata.get('gold_label', 'unknown')\n",
        "    gold_crisis_type = doc.metadata.get('crisis_type', 'unknown')\n",
        "    gold_location = doc.metadata.get('location', 'unknown')\n",
        "    \n",
        "    evaluation_data.append({\n",
        "        'tweet_id': doc.metadata.get('tweet_id', ''),\n",
        "        'text': doc.text[:100],\n",
        "        'predicted_label': predicted_label,\n",
        "        'predicted_confidence': predicted_conf,\n",
        "        'gold_label': gold_label,\n",
        "        'predicted_crisis_type': crisis_type,\n",
        "        'gold_crisis_type': gold_crisis_type,\n",
        "        'predicted_locations': ', '.join(locations) if locations else 'none',\n",
        "        'gold_location': gold_location,\n",
        "        'crisis_type_extracted': crisis_type != 'none_extracted',  # Track if extraction worked\n",
        "        'location_extracted': any(loc != 'unknown' for loc in locations) if locations else False,  # Track if real locations found\n",
        "        'location_match': any(gold_location.lower() in loc.lower() for loc in locations) if locations else False\n",
        "    })\n",
        "\n",
        "df_eval = pd.DataFrame(evaluation_data)"
      ],
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyze the obtained results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:42.827954Z",
          "start_time": "2025-11-05T17:19:42.821883Z"
        }
      },
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PIPELINE PERFORMANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Crisis Classification Performance\n",
        "df_eval['gold_binary'] = df_eval['gold_label'].apply(\n",
        "    lambda x: 'not_related_to_crisis' if 'not_related' in x.lower() or 'irrelevant' in x.lower() \n",
        "    else 'related_to_crisis'\n",
        ")\n",
        "classification_accuracy = (df_eval['predicted_label'] == df_eval['gold_binary']).sum() / len(df_eval) * 100\n",
        "\n",
        "print(f\"\\n1. CRISIS CLASSIFICATION\")\n",
        "print(f\"   Accuracy: {classification_accuracy:.1f}%\")\n",
        "print(f\"   Predicted: {sum(df_eval['predicted_label'] == 'related_to_crisis')} crisis, {sum(df_eval['predicted_label'] == 'not_related_to_crisis')} not crisis\")\n",
        "\n",
        "# 2. Crisis Type Extraction Performance  \n",
        "crisis_type_success = df_eval['crisis_type_extracted'].sum()\n",
        "crisis_type_total = len(df_eval)\n",
        "\n",
        "print(f\"\\n2. CRISIS TYPE EXTRACTION\")\n",
        "print(f\"   Extracted from: {crisis_type_success}/{crisis_type_total} tweets\")\n",
        "print(f\"   Success rate: {crisis_type_success/crisis_type_total*100:.1f}%\")\n",
        "if crisis_type_success > 0:\n",
        "    successful = df_eval[df_eval['crisis_type_extracted'] == True]\n",
        "    types = successful['predicted_crisis_type'].value_counts()\n",
        "    print(f\"   Types found: {', '.join(types.index.tolist())}\")\n",
        "\n",
        "# 3. Location Extraction Performance\n",
        "location_success = df_eval['location_extracted'].sum()\n",
        "location_total = len(df_eval)\n",
        "\n",
        "print(f\"\\n3. LOCATION EXTRACTION\")\n",
        "print(f\"   Extracted from: {location_success}/{location_total} tweets\")\n",
        "print(f\"   Success rate: {location_success/location_total*100:.1f}%\")\n",
        "if location_success > 0:\n",
        "    successful_locs = df_eval[df_eval['location_extracted'] == True]['predicted_locations']\n",
        "    all_locations = set()\n",
        "    for locs in successful_locs:\n",
        "        all_locations.update(locs.split(', '))\n",
        "    print(f\"   Locations found: {len(all_locations)} unique locations\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PIPELINE PERFORMANCE\n",
            "================================================================================\n",
            "\n",
            "1. CRISIS CLASSIFICATION\n",
            "   Accuracy: 100.0%\n",
            "   Predicted: 8 crisis, 0 not crisis\n",
            "\n",
            "2. CRISIS TYPE EXTRACTION\n",
            "   Extracted from: 8/8 tweets\n",
            "   Success rate: 100.0%\n",
            "   Types found: Earthquake, Unknown, Cyclone, Hurricane, Ebola\n",
            "\n",
            "3. LOCATION EXTRACTION\n",
            "   Extracted from: 8/8 tweets\n",
            "   Success rate: 100.0%\n",
            "   Locations found: 4 unique locations\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the evaluation df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-05T17:19:42.851726Z",
          "start_time": "2025-11-05T17:19:42.846634Z"
        }
      },
      "source": [
        "df_eval.head(10)"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "               tweet_id                                               text  \\\n",
              "0  '468077392955990016'  (#Luke 21:11) #MERS: Saudis + 3 more deaths, W...   \n",
              "1  '592955183753203715'  VIDEO REPORT Kathmandu overwhelmed by rubble a...   \n",
              "2  '383790723222364161'  #Earthquake 2013-09-28 02:39:43 (M5.0) EAST OF...   \n",
              "3  '576754146944163840'  RT @WFP_Asia: In contact with #vanuatu to unde...   \n",
              "4  '514627577831768064'  (09/22) ONE WAY TO HELP ODILE VICTIMS  #surfin...   \n",
              "5  '503765744333901824'  Say what you want, but the earthquake that hit...   \n",
              "6  '523225601910775808'  RT @itsmenanice: Read this. From Miasma to #Eb...   \n",
              "7  '451585608612208640'  Guys northem chile really needs a support mess...   \n",
              "\n",
              "     predicted_label  predicted_confidence  \\\n",
              "0  related_to_crisis                  1.00   \n",
              "1  related_to_crisis                  1.00   \n",
              "2  related_to_crisis                  1.00   \n",
              "3  related_to_crisis                  1.00   \n",
              "4  related_to_crisis                  0.98   \n",
              "5  related_to_crisis                  1.00   \n",
              "6  related_to_crisis                  1.00   \n",
              "7  related_to_crisis                  1.00   \n",
              "\n",
              "                                          gold_label predicted_crisis_type  \\\n",
              "0                                     deaths_reports               Unknown   \n",
              "1                             injured_or_dead_people            Earthquake   \n",
              "2                           other_useful_information            Earthquake   \n",
              "3  donation_needs_or_offers_or_volunteering_services               Cyclone   \n",
              "4                           other_useful_information             Hurricane   \n",
              "5                           other_useful_information            Earthquake   \n",
              "6                           other_useful_information                 Ebola   \n",
              "7                     sympathy_and_emotional_support            Earthquake   \n",
              "\n",
              "  gold_crisis_type predicted_locations gold_location  crisis_type_extracted  \\\n",
              "0           Middle             Unknown          2014                   True   \n",
              "1            Nepal               Nepal          2015                   True   \n",
              "2         Pakistan             Unknown          2013                   True   \n",
              "3          Cyclone             Vanuatu          2015                   True   \n",
              "4        Hurricane             Unknown          2014                   True   \n",
              "5       California          California          2014                   True   \n",
              "6            ebola             Unknown          2014                   True   \n",
              "7            Chile             Unknown          2014                   True   \n",
              "\n",
              "   location_extracted  location_match        gold_binary  \n",
              "0                True           False  related_to_crisis  \n",
              "1                True           False  related_to_crisis  \n",
              "2                True           False  related_to_crisis  \n",
              "3                True           False  related_to_crisis  \n",
              "4                True           False  related_to_crisis  \n",
              "5                True           False  related_to_crisis  \n",
              "6                True           False  related_to_crisis  \n",
              "7                True           False  related_to_crisis  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>predicted_label</th>\n",
              "      <th>predicted_confidence</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>predicted_crisis_type</th>\n",
              "      <th>gold_crisis_type</th>\n",
              "      <th>predicted_locations</th>\n",
              "      <th>gold_location</th>\n",
              "      <th>crisis_type_extracted</th>\n",
              "      <th>location_extracted</th>\n",
              "      <th>location_match</th>\n",
              "      <th>gold_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'468077392955990016'</td>\n",
              "      <td>(#Luke 21:11) #MERS: Saudis + 3 more deaths, W...</td>\n",
              "      <td>related_to_crisis</td>\n",
              "      <td>1.00</td>\n",
              "      <td>deaths_reports</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Middle</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>2014</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>related_to_crisis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'592955183753203715'</td>\n",
              "      <td>VIDEO REPORT Kathmandu overwhelmed by rubble a...</td>\n",
              "      <td>related_to_crisis</td>\n",
              "      <td>1.00</td>\n",
              "      <td>injured_or_dead_people</td>\n",
              "      <td>Earthquake</td>\n",
              "      <td>Nepal</td>\n",
              "      <td>Nepal</td>\n",
              "      <td>2015</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>related_to_crisis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'383790723222364161'</td>\n",
              "      <td>#Earthquake 2013-09-28 02:39:43 (M5.0) EAST OF...</td>\n",
              "      <td>related_to_crisis</td>\n",
              "      <td>1.00</td>\n",
              "      <td>other_useful_information</td>\n",
              "      <td>Earthquake</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>2013</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>related_to_crisis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'576754146944163840'</td>\n",
              "      <td>RT @WFP_Asia: In contact with #vanuatu to unde...</td>\n",
              "      <td>related_to_crisis</td>\n",
              "      <td>1.00</td>\n",
              "      <td>donation_needs_or_offers_or_volunteering_services</td>\n",
              "      <td>Cyclone</td>\n",
              "      <td>Cyclone</td>\n",
              "      <td>Vanuatu</td>\n",
              "      <td>2015</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>related_to_crisis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>'514627577831768064'</td>\n",
              "      <td>(09/22) ONE WAY TO HELP ODILE VICTIMS  #surfin...</td>\n",
              "      <td>related_to_crisis</td>\n",
              "      <td>0.98</td>\n",
              "      <td>other_useful_information</td>\n",
              "      <td>Hurricane</td>\n",
              "      <td>Hurricane</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>2014</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>related_to_crisis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>'503765744333901824'</td>\n",
              "      <td>Say what you want, but the earthquake that hit...</td>\n",
              "      <td>related_to_crisis</td>\n",
              "      <td>1.00</td>\n",
              "      <td>other_useful_information</td>\n",
              "      <td>Earthquake</td>\n",
              "      <td>California</td>\n",
              "      <td>California</td>\n",
              "      <td>2014</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>related_to_crisis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>'523225601910775808'</td>\n",
              "      <td>RT @itsmenanice: Read this. From Miasma to #Eb...</td>\n",
              "      <td>related_to_crisis</td>\n",
              "      <td>1.00</td>\n",
              "      <td>other_useful_information</td>\n",
              "      <td>Ebola</td>\n",
              "      <td>ebola</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>2014</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>related_to_crisis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>'451585608612208640'</td>\n",
              "      <td>Guys northem chile really needs a support mess...</td>\n",
              "      <td>related_to_crisis</td>\n",
              "      <td>1.00</td>\n",
              "      <td>sympathy_and_emotional_support</td>\n",
              "      <td>Earthquake</td>\n",
              "      <td>Chile</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>2014</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>related_to_crisis</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary\n",
        "\n",
        "This use case demonstrates the power of **Sieves** for common NLP tasks:\n",
        "\n",
        "✅ **Language Detection** - Automatically filters non-English tweets  \n",
        "✅ **Classification** - Zero-shot classification to identify crisis-related content  \n",
        "✅ **Information Extraction** - Structured extraction of crisis types and locations\n",
        "\n",
        "- [Sieves Documentation](https://sieves.ai/)\n",
        "- [CrisisNLP Dataset](https://crisisnlp.qcri.org/lrec2016/lrec2016.html)\n",
        "- [Sieves GitHub](https://github.com/mantisai/sieves)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".sieves-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
